{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import configparser\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import boto3 \n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('covid19-analytics.config'))\n",
    "\n",
    "KEY = config.get('AWS', 'KEY')\n",
    "SECRET = config.get('AWS', 'SECRET')\n",
    "TARGET_S3 = config.get('S3', 'TARGET_S3')\n",
    "TARGET_OUTPUT_S3 = config.get('S3', 'TARGET_OUTPUT_S3')\n",
    "TARGET_OUTPUT_BUCKET=config.get('S3', 'TARGET_OUTPUT_BUCKET')\n",
    "TARGET_OUTPUT_DIR=config.get('S3', 'TARGET_OUTPUT_DIR')\n",
    "TARGET_REGION = config.get('S3', 'TARGET_REGION')\n",
    "SCHEMA_NAME = config.get('GLUE', 'SCHEMA_NAME')\n",
    "TMP_DIR = config.get('FILE_PATHS', 'TMP_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_S3_CLIENT = boto3.client(\n",
    "    's3', \n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY, \n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "GLUE_CLIENT = boto3.client(\n",
    "    'glue', \n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET,\n",
    "    region_name=TARGET_REGION\n",
    ")\n",
    "\n",
    "ATHENA_CLIENT = boto3.client(\n",
    "    'athena',\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET,\n",
    "    region_name=TARGET_REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_athena_tables(client, database_name) -> list:\n",
    "    tables = []\n",
    "    paginator = client.get_paginator('get_tables')\n",
    "\n",
    "    # Use paginator to handle potentially large number of tables\n",
    "    for page in paginator.paginate(DatabaseName=database_name):\n",
    "        for table in page['TableList']:\n",
    "            tables.append(table['Name'])\n",
    "            \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute athenaa query and retrieve data in all tables\n",
    "def query_athena_and_fetch_results(\n",
    "        athena_client,\n",
    "        s3_client, \n",
    "        database, \n",
    "        query,\n",
    "        table,\n",
    "        output_s3,\n",
    "        output_dir,\n",
    "        output_location,\n",
    "        tmp_dir):\n",
    "    \n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': database},\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': output_location,\n",
    "            'EncryptionConfiguration': {'EncryptionOption': 'SSE_S3'},\n",
    "        }\n",
    "    )\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "\n",
    "    # Loop till query execution is complete\n",
    "    while True:\n",
    "        try:\n",
    "            response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        except ClientError as e:\n",
    "            print(f\"\\nQuery Error: \\n{e}\")\n",
    "            \n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        if state == 'SUCCEEDED':\n",
    "            print(f\"\\n{query_execution_id} query has completed successfuly\")\n",
    "\n",
    "            results_path = f'{output_dir}/{query_execution_id}.csv'\n",
    "            local_filename = f'{tmp_dir}/{table}.csv'\n",
    "\n",
    "            if Path(local_filename).exists():\n",
    "                print(f\"{local_filename} already exists, skip download\")\n",
    "            else:\n",
    "                try:\n",
    "                    s3_client.download_file(output_s3, results_path, local_filename)\n",
    "                    print(f\"\\n{local_filename} downloaded successfuly\")\n",
    "                except ClientError as e:\n",
    "                    print(f\"Download Error: \\n{e}\")\n",
    "                    \n",
    "            try:\n",
    "                s3_client.delete_objects(Bucket=output_s3, Delete={'Objects': [{'Key': results_path}, {'Key': f'{results_path}.metadata'}], 'Quiet': True})\n",
    "            except ClientError as e:\n",
    "                print(f\"S3 cleanup Error: \\n{e}\")\n",
    "\n",
    "            return\n",
    "        \n",
    "        elif state in ['FAILED', 'CANCELLED']:\n",
    "            raise Exception(f\"Query {state.lower()} with reason: {response['QueryExecution']['Status']['StateChangeReason']}\")\n",
    "        else:\n",
    "            print(f\"/n{query_execution_id} query is still running, waiting 3 seconds...\")\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = list_athena_tables(GLUE_CLIENT, SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_table_data(tables, max_workers):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_table = {\n",
    "            executor.submit(\n",
    "                query_athena_and_fetch_results,\n",
    "                athena_client=ATHENA_CLIENT,\n",
    "                s3_client=OUTPUT_S3_CLIENT, \n",
    "                database=SCHEMA_NAME,  \n",
    "                query=f'SELECT * FROM \"{TABLE}\";',\n",
    "                table=TABLE,\n",
    "                output_s3=TARGET_OUTPUT_S3,\n",
    "                output_dir=TARGET_OUTPUT_DIR,\n",
    "                output_location=TARGET_OUTPUT_BUCKET,\n",
    "                tmp_dir=TMP_DIR\n",
    "            ): TABLE for TABLE in tables\n",
    "        }\n",
    "        for future in as_completed(future_to_table):\n",
    "            table = future_to_table[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download csv for {table}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nef174cc6-ae56-4ba5-8927-e54999455dcc query is still running, waiting 3 seconds...\n",
      "/nb03043f3-4496-4267-b89f-64dbe7ab6a5e query is still running, waiting 3 seconds...\n",
      "/n9aca16b3-3c12-4dea-9ff2-a661875dcb15 query is still running, waiting 3 seconds...\n",
      "\n",
      "ef174cc6-ae56-4ba5-8927-e54999455dcc query has completed successfuly\n",
      "\n",
      "9aca16b3-3c12-4dea-9ff2-a661875dcb15 query has completed successfuly\n",
      "\n",
      "b03043f3-4496-4267-b89f-64dbe7ab6a5e query has completed successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/static-datacountrycode.csv downloaded successfuly\n",
      "/n91df7cbc-93c0-408c-a6c4-4cb09c12cb65 query is still running, waiting 3 seconds...\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/hospital-bedsjson.csv downloaded successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/enigma_jhu.csv downloaded successfuly\n",
      "/nf435fc78-674e-4f4c-9294-b3f762a4bdb5 query is still running, waiting 3 seconds...\n",
      "/n1328c641-93e8-4e95-9b12-bb592feb7b11 query is still running, waiting 3 seconds...\n",
      "\n",
      "91df7cbc-93c0-408c-a6c4-4cb09c12cb65 query has completed successfuly\n",
      "\n",
      "f435fc78-674e-4f4c-9294-b3f762a4bdb5 query has completed successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/static-datacountypopulation.csv downloaded successfuly\n",
      "\n",
      "1328c641-93e8-4e95-9b12-bb592feb7b11 query has completed successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/static-datastate_abv.csv downloaded successfuly\n",
      "/n0d25f2cf-c62d-4500-886e-e7aef0824c56 query is still running, waiting 3 seconds...\n",
      "/n8d0302ea-d35a-40eb-9d3a-c5d64fd4dad1 query is still running, waiting 3 seconds...\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/testing-datastates_daily.csv downloaded successfuly\n",
      "/n8e5c176e-077e-4749-b0c5-cb516e884d5c query is still running, waiting 3 seconds...\n",
      "\n",
      "0d25f2cf-c62d-4500-886e-e7aef0824c56 query has completed successfuly\n",
      "\n",
      "8d0302ea-d35a-40eb-9d3a-c5d64fd4dad1 query has completed successfuly\n",
      "\n",
      "8e5c176e-077e-4749-b0c5-cb516e884d5c query has completed successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/us-totalus_total_latest.csv downloaded successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/testing-dataus_daily.csv downloaded successfuly\n",
      "/neb89cdf8-5f64-49f1-8813-4aaf1b48872f query is still running, waiting 3 seconds...\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/us_county.csv downloaded successfuly\n",
      "\n",
      "eb89cdf8-5f64-49f1-8813-4aaf1b48872f query has completed successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/us_states.csv downloaded successfuly\n"
     ]
    }
   ],
   "source": [
    "download_table_data(tables, max_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma_jhu = pd.read_csv(f'{TMP_DIR}/enigma_jhu.csv')\n",
    "\n",
    "testing_data_states_daily = pd.read_csv(f'{TMP_DIR}/testing-datastates_daily.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "factCovid_1 = enigma_jhu[['fips', 'province_state', 'country_region', 'confirmed', 'deaths', 'recovered', 'active' ]]\n",
    "factCovid_2 = testing_data_states_daily[['fips', 'date', 'positive', 'negative', 'hospitalizedcurrently', 'hospitalized', 'hospitalizeddischarged' ]]\n",
    "factCovid = pd.merge(factCovid_1, factCovid_2, on='fips', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimHospital = pd.read_csv(f'{TMP_DIR}/hospital-bedsjson.csv')\n",
    "dimHospital =  dimHospital[['fips', 'state_name', 'latitude', 'longtitude', 'hq_address', 'hospital_name', 'hospital_type', 'hq_city', 'hq_state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate = pd.read_csv(f'{TMP_DIR}/testing-datastates_daily.csv')\n",
    "dimDate = dimDate[['fips', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate['date'] = pd.to_datetime(dimDate['date'], format='%Y%m%d')\n",
    "dimDate['year'] = dimDate['date'].dt.year\n",
    "dimDate['month'] = dimDate['date'].dt.month\n",
    "dimDate[\"day_of_week\"] = dimDate['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Join\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "enigma_jhu = spark.read.csv(f'{TMP_DIR}/enigma_jhu.csv', header=True, inferSchema=True)\n",
    "ny_times_us_county = spark.read.csv(f'{TMP_DIR}/us_county.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion_1 = enigma_jhu.select('fips', 'province_state', 'country_region', 'latitude', 'longitude')\n",
    "dimRegion_2 = ny_times_us_county.select('fips', 'county', 'state')\n",
    "\n",
    "dimRegion_1 = dimRegion_1.repartition(4, 'fips')\n",
    "dimRegion_2 = dimRegion_2.repartition(4, 'fips')\n",
    "dimRegion_2 = dimRegion_2.withColumnRenamed('fips', 'fips2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion = dimRegion_1.join(dimRegion_2, dimRegion_1[\"fips\"] == dimRegion_2[\"fips2\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion = dimRegion.drop('fips2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=========>        (1 + 1) / 2][Stage 35:>                 (0 + 1) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dimRegion.coalesce(1).write.csv(f'{TMP_DIR}/dimRegion.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv {TMP_DIR}/dimRegion.csv/part-00000* {TMP_DIR}/dimRegions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -r -f {TMP_DIR}/dimRegion.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'P3YHPK93WC6TW6QH',\n",
       "  'HostId': '4w691pFhUXZk2hCM2EJEYThe6ohvKfC14CU5p13omxU1Xh5VMS3tagM7CFR0Mal8f7FO6Qglclo=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '4w691pFhUXZk2hCM2EJEYThe6ohvKfC14CU5p13omxU1Xh5VMS3tagM7CFR0Mal8f7FO6Qglclo=',\n",
       "   'x-amz-request-id': 'P3YHPK93WC6TW6QH',\n",
       "   'date': 'Wed, 31 Jul 2024 11:12:04 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"9e84c6b6a22cee482ae52a6000e415f0\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"9e84c6b6a22cee482ae52a6000e415f0\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factCovid.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/factCovid.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'R9Y2BYVT8PKWFE71',\n",
       "  'HostId': 'td29p2tDF4BuCnNsZUo836BXn9j8g7Pv3fkOUCTElet4mDdMM7edAUdyPvoDWpvwjnTpRF9h5oM=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'td29p2tDF4BuCnNsZUo836BXn9j8g7Pv3fkOUCTElet4mDdMM7edAUdyPvoDWpvwjnTpRF9h5oM=',\n",
       "   'x-amz-request-id': 'R9Y2BYVT8PKWFE71',\n",
       "   'date': 'Wed, 31 Jul 2024 11:12:05 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"b598ab67bb0fa5268bf9a99a62622af9\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"b598ab67bb0fa5268bf9a99a62622af9\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimHospital.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimHospital.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'R9YDHBBASGD48V9D',\n",
       "  'HostId': 'RSlMfFx6Od5kTnyiAbDplad0vJsgwzRU7MJ/qGXIMEQ/0CZQ05Sr7UgEFNCue915OLsDZdEyKYM=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'RSlMfFx6Od5kTnyiAbDplad0vJsgwzRU7MJ/qGXIMEQ/0CZQ05Sr7UgEFNCue915OLsDZdEyKYM=',\n",
       "   'x-amz-request-id': 'R9YDHBBASGD48V9D',\n",
       "   'date': 'Wed, 31 Jul 2024 11:12:05 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"82acca58660c7b1b726c6874ad1aced0\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"82acca58660c7b1b726c6874ad1aced0\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimDate.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimDate.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_S3_CLIENT.upload_file(\n",
    "    f'{TMP_DIR}/dimRegions.csv',\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimRegions.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -r -f {TMP_DIR}/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
