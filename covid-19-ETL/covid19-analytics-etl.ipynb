{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('covid19-analytics.config'))\n",
    "\n",
    "KEY = config.get('AWS', 'KEY')\n",
    "SECRET = config.get('AWS', 'SECRET')\n",
    "TARGET_S3 = config.get('S3', 'TARGET_S3')\n",
    "TARGET_OUTPUT_S3 = config.get('S3', 'TARGET_OUTPUT_S3')\n",
    "TARGET_OUTPUT_BUCKET=config.get('S3', 'TARGET_OUTPUT_BUCKET')\n",
    "TARGET_OUTPUT_DIR=config.get('S3', 'TARGET_OUTPUT_DIR')\n",
    "TARGET_REGION = config.get('S3', 'TARGET_REGION')\n",
    "SCHEMA_NAME = config.get('GLUE', 'SCHEMA_NAME')\n",
    "TMP_DIR = config.get('FILE_PATHS', 'TMP_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_S3_CLIENT = boto3.client(\n",
    "    's3', \n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY, \n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "GLUE_CLIENT = boto3.client(\n",
    "    'glue', \n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET,\n",
    "    region_name=TARGET_REGION\n",
    ")\n",
    "\n",
    "ATHENA_CLIENT = boto3.client(\n",
    "    'athena',\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET,\n",
    "    region_name=TARGET_REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_athena_tables(client, database_name) -> list:\n",
    "    tables = []\n",
    "    paginator = client.get_paginator('get_tables')\n",
    "\n",
    "    # Use paginator to handle potentially large number of tables\n",
    "    for page in paginator.paginate(DatabaseName=database_name):\n",
    "        for table in page['TableList']:\n",
    "            tables.append(table['Name'])\n",
    "            \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute athenaa query and retrieve data in all tables\n",
    "def query_athena_and_fetch_results(\n",
    "        athena_client,\n",
    "        s3_client, \n",
    "        database, \n",
    "        query,\n",
    "        table,\n",
    "        output_s3,\n",
    "        output_dir,\n",
    "        output_location,\n",
    "        tmp_dir):\n",
    "    \n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': database},\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': output_location,\n",
    "            'EncryptionConfiguration': {'EncryptionOption': 'SSE_S3'},\n",
    "        }\n",
    "    )\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "\n",
    "    # Loop till query execution is complete\n",
    "    while True:\n",
    "        try:\n",
    "            response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        except ClientError as e:\n",
    "            print(f\"\\nQuery Error: \\n{e}\")\n",
    "            \n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        if state == 'SUCCEEDED':\n",
    "            print(f\"\\n{query_execution_id} query has completed successfuly\")\n",
    "\n",
    "            results_path = f'{output_dir}/{query_execution_id}.csv'\n",
    "            local_filename = f'{tmp_dir}/{table}.csv'\n",
    "\n",
    "            if Path(local_filename).exists():\n",
    "                print(f\"{local_filename} already exists, skip download\")\n",
    "            else:\n",
    "                try:\n",
    "                    s3_client.download_file(output_s3, results_path, local_filename)\n",
    "                    print(f\"\\n{local_filename} downloaded successfuly\")\n",
    "                except ClientError as e:\n",
    "                    print(f\"Download Error: \\n{e}\")\n",
    "                    \n",
    "            try:\n",
    "                s3_client.delete_objects(Bucket=output_s3, Delete={'Objects': [{'Key': results_path}, {'Key': f'{results_path}.metadata'}], 'Quiet': True})\n",
    "            except ClientError as e:\n",
    "                print(f\"S3 cleanup Error: \\n{e}\")\n",
    "\n",
    "            return\n",
    "        \n",
    "        elif state in ['FAILED', 'CANCELLED']:\n",
    "            raise Exception(f\"Query {state.lower()} with reason: {response['QueryExecution']['Status']['StateChangeReason']}\")\n",
    "        else:\n",
    "            print(f\"/n{query_execution_id} query is still running, waiting 3 seconds...\")\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = list_athena_tables(GLUE_CLIENT, SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n0daeff51-6b62-4abb-976a-3911601a7358 query is still running, waiting 3 seconds...\n",
      "/n0daeff51-6b62-4abb-976a-3911601a7358 query is still running, waiting 3 seconds...\n",
      "\n",
      "0daeff51-6b62-4abb-976a-3911601a7358 query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/enigma_jhu.csv already exists, skip download\n",
      "/n088198d8-51f8-4661-91be-3a6682bfbd57 query is still running, waiting 3 seconds...\n",
      "\n",
      "088198d8-51f8-4661-91be-3a6682bfbd57 query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/hospital-bedsjson.csv already exists, skip download\n",
      "/n204334c4-fd24-4029-bb77-65376fb82fc9 query is still running, waiting 3 seconds...\n",
      "\n",
      "204334c4-fd24-4029-bb77-65376fb82fc9 query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/static-datacountrycode.csv already exists, skip download\n",
      "/ne9651ebb-dc28-42ff-a1dc-d0822e4f9f7e query is still running, waiting 3 seconds...\n",
      "\n",
      "e9651ebb-dc28-42ff-a1dc-d0822e4f9f7e query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/static-datacountypopulation.csv already exists, skip download\n",
      "/n690cec5a-87e3-47cd-9bae-394b8ddac7fb query is still running, waiting 3 seconds...\n",
      "\n",
      "690cec5a-87e3-47cd-9bae-394b8ddac7fb query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/static-datastate_abv.csv already exists, skip download\n",
      "/n4789c9e1-c11b-40b8-be44-b52d7387c9fe query is still running, waiting 3 seconds...\n",
      "\n",
      "4789c9e1-c11b-40b8-be44-b52d7387c9fe query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/testing-datastates_daily.csv already exists, skip download\n",
      "/n7231c119-df3e-4fcd-8a26-08a4e1bfc2fb query is still running, waiting 3 seconds...\n",
      "\n",
      "7231c119-df3e-4fcd-8a26-08a4e1bfc2fb query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/testing-dataus_daily.csv already exists, skip download\n",
      "/n70ec2ab1-985e-4f31-90aa-367eb9bad364 query is still running, waiting 3 seconds...\n",
      "\n",
      "70ec2ab1-985e-4f31-90aa-367eb9bad364 query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/us-totalus_total_latest.csv already exists, skip download\n",
      "/na0a9f448-6305-4e34-92f9-a2f67a93a11e query is still running, waiting 3 seconds...\n",
      "\n",
      "a0a9f448-6305-4e34-92f9-a2f67a93a11e query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/us_county.csv already exists, skip download\n",
      "/n88fa99ea-1b6f-419f-8f6b-74c31f458b93 query is still running, waiting 3 seconds...\n",
      "\n",
      "88fa99ea-1b6f-419f-8f6b-74c31f458b93 query has completed successfuly\n",
      "/home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/us_states.csv already exists, skip download\n"
     ]
    }
   ],
   "source": [
    "for TABLE in tables:\n",
    "    QUERY = f'SELECT * FROM \"{TABLE}\";'\n",
    "    try:\n",
    "        query_athena_and_fetch_results(\n",
    "            athena_client=ATHENA_CLIENT,\n",
    "            s3_client=OUTPUT_S3_CLIENT, \n",
    "            database=SCHEMA_NAME,  \n",
    "            query=QUERY,\n",
    "            table=TABLE,\n",
    "            output_s3=TARGET_OUTPUT_S3,\n",
    "            output_dir=TARGET_OUTPUT_DIR,\n",
    "            output_location=TARGET_OUTPUT_BUCKET,\n",
    "            tmp_dir=TMP_DIR\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download csv for table {TABLE}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/enigma_jhu.csv into enigma_jhu_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/hospital-bedsjson.csv into hospital-bedsjson_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/static-datacountrycode.csv into static-datacountrycode_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/static-datacountypopulation.csv into static-datacountypopulation_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/static-datastate_abv.csv into static-datastate_abv_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/testing-datastates_daily.csv into testing-datastates_daily_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/testing-dataus_daily.csv into testing-dataus_daily_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/us-totalus_total_latest.csv into us-totalus_total_latest_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/us_county.csv into us_county_df dataframe\n",
      "\n",
      "Loaded /home/oseloka/aws_tools/aws-etl-projects/covid-19-ETL/tmp/us_states.csv into us_states_df dataframe\n"
     ]
    }
   ],
   "source": [
    "# iterate through the tmp folder\n",
    "from pathlib import Path\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for table in tables:\n",
    "    for file in Path(TMP_DIR).iterdir():\n",
    "        if table in file.name:\n",
    "            dataframes[f'{table}_df'] = pd.read_csv(file)\n",
    "            print(f\"\\nLoaded {file} into {table}_df dataframe\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
