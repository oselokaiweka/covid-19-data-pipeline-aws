{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import configparser\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import boto3\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('covid19-analytics.config'))\n",
    "\n",
    "KEY = config.get('AWS', 'KEY')\n",
    "SECRET = config.get('AWS', 'SECRET')\n",
    "\n",
    "TARGET_S3 = config.get('S3', 'TARGET_S3')\n",
    "TARGET_OUTPUT_S3 = config.get('S3', 'TARGET_OUTPUT_S3')\n",
    "TARGET_OUTPUT_BUCKET=config.get('S3', 'TARGET_OUTPUT_BUCKET')\n",
    "TARGET_OUTPUT_DIR=config.get('S3', 'TARGET_OUTPUT_DIR')\n",
    "TARGET_REGION = config.get('S3', 'TARGET_REGION')\n",
    "SCHEMA_NAME = config.get('GLUE', 'SCHEMA_NAME')\n",
    "TMP_DIR = config.get('FILE_PATHS', 'TMP_DIR')\n",
    "\n",
    "DWH_CLUSTER_TYPE = config.get('DWH', 'DWH_CLUSTER_TYPE')\n",
    "DWH_NUM_NODES = config.get('DWH', 'DWH_NUM_NODES')\n",
    "DWH_NODE_TYPE = config.get('DWH', 'DWH_NODE_TYPE')\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_DB = config.get('DWH', 'DWH_DB')\n",
    "DWH_DB_USER = config.get('DWH', 'DWH_DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('DWH', 'DWH_DB_PASSWORD')\n",
    "DWH_PORT = config.get('DWH', 'DWH_PORT')\n",
    "DWH_IAM_ROLE_NAME = config.get('DWH', 'DWH_IAM_ROLE_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>single-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>covid19-redshift-cluster-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>covid19-redshift-db-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>oseloka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Oseloka1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>redshift-tutorial-s3-access-role</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param                             Value\n",
       "0        DWH_CLUSTER_TYPE                       single-node\n",
       "1           DWH_NUM_NODES                                 1\n",
       "2           DWH_NODE_TYPE                         dc2.large\n",
       "3  DWH_CLUSTER_IDENTIFIER        covid19-redshift-cluster-1\n",
       "4                  DWH_DB             covid19-redshift-db-1\n",
       "5             DWH_DB_USER                           oseloka\n",
       "6         DWH_DB_PASSWORD                          Oseloka1\n",
       "7                DWH_PORT                              5439\n",
       "8       DWH_IAM_ROLE_NAME  redshift-tutorial-s3-access-role"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        'Param':\n",
    "            [\n",
    "                'DWH_CLUSTER_TYPE',\n",
    "                'DWH_NUM_NODES',\n",
    "                'DWH_NODE_TYPE',\n",
    "                'DWH_CLUSTER_IDENTIFIER',\n",
    "                'DWH_DB',\n",
    "                'DWH_DB_USER',\n",
    "                'DWH_DB_PASSWORD',\n",
    "                'DWH_PORT',\n",
    "                'DWH_IAM_ROLE_NAME'\n",
    "            ],\n",
    "        'Value':\n",
    "            [\n",
    "                DWH_CLUSTER_TYPE,\n",
    "                DWH_NUM_NODES,\n",
    "                DWH_NODE_TYPE,\n",
    "                DWH_CLUSTER_IDENTIFIER,\n",
    "                DWH_DB,\n",
    "                DWH_DB_USER,\n",
    "                DWH_DB_PASSWORD,\n",
    "                DWH_PORT,\n",
    "                DWH_IAM_ROLE_NAME\n",
    "\n",
    "            ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_S3_CLIENT = boto3.client(\n",
    "    's3', \n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY, \n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "GLUE_CLIENT = boto3.client(\n",
    "    'glue', \n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET,\n",
    "    region_name=TARGET_REGION\n",
    ")\n",
    "\n",
    "ATHENA_CLIENT = boto3.client(\n",
    "    'athena',\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET,\n",
    "    region_name=TARGET_REGION,\n",
    ")\n",
    "\n",
    "redshift_client = boto3.client(\n",
    "    'redshift',\n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "ec2_client = boto3.resource(\n",
    "    'ec2',\n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "iam_client = boto3.client(\n",
    "    'iam',\n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshiftToS3_roleArn = iam_client.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_athena_tables(client, database_name) -> list:\n",
    "    tables = []\n",
    "    paginator = client.get_paginator('get_tables')\n",
    "\n",
    "    # Use paginator to handle potentially large number of tables\n",
    "    for page in paginator.paginate(DatabaseName=database_name):\n",
    "        for table in page['TableList']:\n",
    "            tables.append(table['Name'])\n",
    "            \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute athena query and retrieve data in all tables\n",
    "def query_athena_and_fetch_results(\n",
    "        athena_client,\n",
    "        s3_client, \n",
    "        database, \n",
    "        query,\n",
    "        table,\n",
    "        output_s3,\n",
    "        output_dir,\n",
    "        output_location,\n",
    "        tmp_dir):\n",
    "    \n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': database},\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': output_location,\n",
    "            'EncryptionConfiguration': {'EncryptionOption': 'SSE_S3'},\n",
    "        }\n",
    "    )\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "\n",
    "    # Loop till query execution is complete\n",
    "    while True:\n",
    "        try:\n",
    "            response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        except ClientError as e:\n",
    "            print(f\"\\nQuery Error: \\n{e}\")\n",
    "            \n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        if state == 'SUCCEEDED':\n",
    "            print(f\"\\n{query_execution_id} query has completed successfuly\")\n",
    "\n",
    "            results_path = f'{output_dir}/{query_execution_id}.csv'\n",
    "            local_filename = f'{tmp_dir}/{table}.csv'\n",
    "\n",
    "            if Path(local_filename).exists():\n",
    "                print(f\"{local_filename} already exists, skip download\")\n",
    "            else:\n",
    "                try:\n",
    "                    s3_client.download_file(output_s3, results_path, local_filename)\n",
    "                    print(f\"\\n{local_filename} downloaded successfuly\")\n",
    "                except ClientError as e:\n",
    "                    print(f\"Download Error: \\n{e}\")\n",
    "                    \n",
    "            try:\n",
    "                s3_client.delete_objects(Bucket=output_s3, Delete={'Objects': [{'Key': results_path}, {'Key': f'{results_path}.metadata'}], 'Quiet': True})\n",
    "            except ClientError as e:\n",
    "                print(f\"S3 cleanup Error: \\n{e}\")\n",
    "\n",
    "            return\n",
    "        \n",
    "        elif state in ['FAILED', 'CANCELLED']:\n",
    "            raise Exception(f\"Query {state.lower()} with reason: {response['QueryExecution']['Status']['StateChangeReason']}\")\n",
    "        else:\n",
    "            print(f\"/n{query_execution_id} query is still running, waiting 3 seconds...\")\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = list_athena_tables(GLUE_CLIENT, SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_table_data(tables, max_workers):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_table = {\n",
    "            executor.submit(\n",
    "                query_athena_and_fetch_results,\n",
    "                athena_client=ATHENA_CLIENT,\n",
    "                s3_client=OUTPUT_S3_CLIENT, \n",
    "                database=SCHEMA_NAME,  \n",
    "                query=f'SELECT * FROM \"{TABLE}\";',\n",
    "                table=TABLE,\n",
    "                output_s3=TARGET_OUTPUT_S3,\n",
    "                output_dir=TARGET_OUTPUT_DIR,\n",
    "                output_location=TARGET_OUTPUT_BUCKET,\n",
    "                tmp_dir=TMP_DIR\n",
    "            ): TABLE for TABLE in tables\n",
    "        }\n",
    "        for future in as_completed(future_to_table):\n",
    "            table = future_to_table[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download csv for {table}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ne81c4c6e-07cd-4d34-a6ae-c1351bcaff46 query is still running, waiting 3 seconds...\n",
      "/nf20f41af-6694-4598-9fb3-b91aafef7005 query is still running, waiting 3 seconds...\n",
      "/nec818b20-ace3-4fb9-92cf-9415b9b56e25 query is still running, waiting 3 seconds...\n",
      "\n",
      "e81c4c6e-07cd-4d34-a6ae-c1351bcaff46 query has completed successfuly\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/hospital-bedsjson.csv already exists, skip download\n",
      "/nf20f41af-6694-4598-9fb3-b91aafef7005 query is still running, waiting 3 seconds...\n",
      "\n",
      "ec818b20-ace3-4fb9-92cf-9415b9b56e25 query has completed successfuly\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/static-datacountrycode.csv already exists, skip download\n",
      "/n91db549b-93e2-4208-9efd-3f1d99881fd2 query is still running, waiting 3 seconds...\n",
      "/ne0c4e8b5-bdf7-440e-b5fe-66465c6a516e query is still running, waiting 3 seconds...\n",
      "\n",
      "f20f41af-6694-4598-9fb3-b91aafef7005 query has completed successfuly\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/enigma_jhu.csv already exists, skip download\n",
      "/n3477fb8f-e57f-483c-b99b-caa5ee0adc73 query is still running, waiting 3 seconds...\n",
      "\n",
      "91db549b-93e2-4208-9efd-3f1d99881fd2 query has completed successfuly\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/static-datacountypopulation.csv already exists, skip download\n",
      "\n",
      "e0c4e8b5-bdf7-440e-b5fe-66465c6a516e query has completed successfuly\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/static-datastate_abv.csv already exists, skip download\n",
      "/n62f8962d-de85-40a0-9ba6-5790d4989ce1 query is still running, waiting 3 seconds...\n",
      "/n0abd6f17-4b07-495a-80c2-7b5f2afd5b60 query is still running, waiting 3 seconds...\n",
      "\n",
      "3477fb8f-e57f-483c-b99b-caa5ee0adc73 query has completed successfuly\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/testing-datastates_daily.csv already exists, skip download\n",
      "\n",
      "62f8962d-de85-40a0-9ba6-5790d4989ce1 query has completed successfuly\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/testing-dataus_daily.csv already exists, skip download\n",
      "\n",
      "0abd6f17-4b07-495a-80c2-7b5f2afd5b60 query has completed successfuly\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/us-totalus_total_latest.csv already exists, skip download\n",
      "/n561408d0-d475-4a69-a444-c4977a0f76f7 query is still running, waiting 3 seconds...\n",
      "/nab9709f5-3564-4595-87f0-6b1498034379 query is still running, waiting 3 seconds...\n",
      "\n",
      "561408d0-d475-4a69-a444-c4977a0f76f7 query has completed successfuly\n",
      "\n",
      "ab9709f5-3564-4595-87f0-6b1498034379 query has completed successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/us_county.csv downloaded successfuly\n",
      "\n",
      "/workspaces/covid-19-data-pipeline-aws/covid-19-ETL/tmp/us_states.csv downloaded successfuly\n"
     ]
    }
   ],
   "source": [
    "download_table_data(tables, max_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma_jhu = pd.read_csv(f'{TMP_DIR}/enigma_jhu.csv')\n",
    "\n",
    "testing_data_states_daily = pd.read_csv(f'{TMP_DIR}/testing-datastates_daily.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "factCovid_1 = enigma_jhu[['fips', 'province_state', 'country_region', 'confirmed', 'deaths', 'recovered', 'active' ]]\n",
    "factCovid_2 = testing_data_states_daily[['fips', 'date', 'positive', 'negative', 'hospitalizedcurrently', 'hospitalized', 'hospitalizeddischarged' ]]\n",
    "factCovid = pd.merge(factCovid_1, factCovid_2, on='fips', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimHospital = pd.read_csv(f'{TMP_DIR}/hospital-bedsjson.csv')\n",
    "dimHospital =  dimHospital[['fips', 'state_name', 'latitude', 'longtitude', 'hq_address', 'hospital_name', 'hospital_type', 'hq_city', 'hq_state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate = pd.read_csv(f'{TMP_DIR}/testing-datastates_daily.csv')\n",
    "dimDate = dimDate[['fips', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate['date'] = pd.to_datetime(dimDate['date'], format='%Y%m%d')\n",
    "dimDate['year'] = dimDate['date'].dt.year\n",
    "dimDate['month'] = dimDate['date'].dt.month\n",
    "dimDate[\"day_of_week\"] = dimDate['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 17:59:35 WARN Utils: Your hostname, codespaces-595706 resolves to a loopback address: 127.0.0.1; using 10.0.0.100 instead (on interface eth0)\n",
      "24/08/08 17:59:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/08 17:59:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Join\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "enigma_jhu = spark.read.csv(\n",
    "    f'{TMP_DIR}/enigma_jhu.csv', \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "ny_times_us_county = spark.read.csv(\n",
    "    f'{TMP_DIR}/us_county.csv', \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 17:59:52 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "dimRegion_1 = enigma_jhu.select('fips', 'province_state', 'country_region', 'latitude', 'longitude')\n",
    "dimRegion_2 = ny_times_us_county.select('fips', 'county', 'state')\n",
    "\n",
    "dimRegion_1 = dimRegion_1.repartition(4, 'fips')\n",
    "dimRegion_2 = dimRegion_2.repartition(4, 'fips')\n",
    "dimRegion_2 = dimRegion_2.withColumnRenamed('fips', 'fips2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion = dimRegion_1.join(\n",
    "    dimRegion_2, \n",
    "    dimRegion_1[\"fips\"] == dimRegion_2[\"fips2\"], \n",
    "    \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion = dimRegion.drop('fips2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dimRegion.coalesce(1).write.csv(f'{TMP_DIR}/dimRegion.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv {TMP_DIR}/dimRegion.csv/part-00000* {TMP_DIR}/dimRegions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -r -f {TMP_DIR}/dimRegion.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'CBJX6WWKBMR5XQW4',\n",
       "  'HostId': 'ZGwm+CzQH6aihz50vsCY9SOs6Hv7EI2w6M+4hzcBAzqS7RbfTITYl1zstuUpiwjXtXsUKad0sX9+jqBpVNh7kA==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'ZGwm+CzQH6aihz50vsCY9SOs6Hv7EI2w6M+4hzcBAzqS7RbfTITYl1zstuUpiwjXtXsUKad0sX9+jqBpVNh7kA==',\n",
       "   'x-amz-request-id': 'CBJX6WWKBMR5XQW4',\n",
       "   'date': 'Thu, 08 Aug 2024 18:00:29 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"9e84c6b6a22cee482ae52a6000e415f0\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"9e84c6b6a22cee482ae52a6000e415f0\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factCovid.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/factCovid.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'GDC26NWKVBCGJ29S',\n",
       "  'HostId': 'nVU0FJhN3THhVaKj4yTpFzwp1n21p+7/pMmyWnc+K+Xt25m/t3otIH3Cpt9er4PHhF8wqUwgPSM1KPCFQ2c3PQ==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'nVU0FJhN3THhVaKj4yTpFzwp1n21p+7/pMmyWnc+K+Xt25m/t3otIH3Cpt9er4PHhF8wqUwgPSM1KPCFQ2c3PQ==',\n",
       "   'x-amz-request-id': 'GDC26NWKVBCGJ29S',\n",
       "   'date': 'Thu, 08 Aug 2024 18:00:33 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"b598ab67bb0fa5268bf9a99a62622af9\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"b598ab67bb0fa5268bf9a99a62622af9\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimHospital.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimHospital.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'DV82NNBSGENM0429',\n",
       "  'HostId': 'vOwxx0YSIuqqqtPV2TLMEYco43k/WS8syogV5OEhhvx2orq78FXd+v3FnKaToGTUoCUbecPeEi9YrKQsrYnaeQ==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'vOwxx0YSIuqqqtPV2TLMEYco43k/WS8syogV5OEhhvx2orq78FXd+v3FnKaToGTUoCUbecPeEi9YrKQsrYnaeQ==',\n",
       "   'x-amz-request-id': 'DV82NNBSGENM0429',\n",
       "   'date': 'Thu, 08 Aug 2024 18:00:37 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"82acca58660c7b1b726c6874ad1aced0\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"82acca58660c7b1b726c6874ad1aced0\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimDate.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimDate.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_S3_CLIENT.upload_file(\n",
    "    f'{TMP_DIR}/dimRegions.csv',\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimRegions.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -r -f {TMP_DIR}/* # Cleanup tmp directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"dimDate\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" INTEGER,\n",
      "  \"date\" TIMESTAMP,\n",
      "  \"year\" INTEGER,\n",
      "  \"month\" INTEGER,\n",
      "  \"day_of_week\" INTEGER\n",
      ");\n",
      "CREATE TABLE \"dimRegion\" (\n",
      "\"index\" INTEGER,\n",
      "\"fips\" INTEGER,\n",
      "\"province_state\" TEXT,\n",
      "\"country_region\" TEXT,\n",
      "\"latitude\" REAL,\n",
      "\"longitude\" REAL,\n",
      "\"county\" TEXT,\n",
      "\"state\" TEXT\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "# Construct CREATE TABLE SQL dynamically from pandas dataframe\n",
    "factCovid_sql = f\"{pd.io.sql.get_schema(factCovid.reset_index(), 'factCovid')};\"\n",
    "dimHospital_sql = f\"{pd.io.sql.get_schema(dimHospital.reset_index(), 'dimHospital')};\"\n",
    "dimDate_sql = f\"{pd.io.sql.get_schema(dimDate.reset_index(), 'dimDate')};\"\n",
    "print(dimDate_sql)\n",
    "\n",
    "# Prepare spark datafram dimRegion to match redshift database table structure\n",
    "# Define a spark window specificaton\n",
    "window_spec = Window.orderBy(\"fips\")\n",
    "\n",
    "# Add a sequential index column\n",
    "dimRegion = dimRegion.withColumn(\"index\", row_number().over(window_spec))\n",
    "\n",
    "# Reorder columns to have index as first column\n",
    "dimRegion = dimRegion.select([\"index\"] + [col for col in dimRegion.columns if col != \"index\"])\n",
    "\n",
    "# Mapping Spark data types to SQL data types\n",
    "data_type_mapping = {\n",
    "    \"STRING\": \"TEXT\",\n",
    "    \"DOUBLE\": \"REAL\",\n",
    "    \"LONG\": \"INTEGER\",\n",
    "    \"INT\": \"INTEGER\"\n",
    "}\n",
    "\n",
    "# Construct CREATE TABLE SQL dynamically from spark dataframe\n",
    "columns = \",\\n\".join(\n",
    "    [f'\"{field.name}\" {data_type_mapping.get(field.dataType.simpleString().upper(), \"TEXT\")}' for field in dimRegion.schema.fields]\n",
    ")\n",
    "dimRegion_sql = f'CREATE TABLE \"dimRegion\" (\\n{columns}\\n);'\n",
    "print(dimRegion_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_redshift_props(props):\n",
    "    pd.set_option('display.max.colwidth', 0)\n",
    "    keysToShow = ['ClusterIdentifier', 'ClusterStatus', 'NodeType', 'NumberOfNodes', 'DBName', 'MasterUsername', 'Endpoint', 'VpcId']\n",
    "    x = [(k, v) for k, v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=['Parameter', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method implements retries while obtaining redshift properties in case creating cluster is yet complete\n",
    "def get_redshift_props(redshift_client, cluster_identifier):\n",
    "    retries = 30\n",
    "    retry_delay = 30 # Delay between retries in seconds\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            clusterProps = redshift_client.describe_clusters(ClusterIdentifier=cluster_identifier)['Clusters'][0]\n",
    "            return clusterProps\n",
    "        except redshift_client.exceptions.ClusterNotFoundFault as e:\n",
    "            if attempt < retries -1:\n",
    "                print(f\"Cluster '{cluster_identifier}' not found. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise e # Raise the last exception if the retries are exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ClusterIdentifier': 'covid19-redshift-cluster-1',\n",
       " 'NodeType': 'dc2.large',\n",
       " 'ClusterStatus': 'available',\n",
       " 'ClusterAvailabilityStatus': 'Available',\n",
       " 'MasterUsername': 'oseloka',\n",
       " 'DBName': 'covid19-redshift-db-1',\n",
       " 'Endpoint': {'Address': 'covid19-redshift-cluster-1.covkciolfldm.us-east-2.redshift.amazonaws.com',\n",
       "  'Port': 5439},\n",
       " 'ClusterCreateTime': datetime.datetime(2024, 8, 8, 16, 45, 51, 892000, tzinfo=tzlocal()),\n",
       " 'AutomatedSnapshotRetentionPeriod': 1,\n",
       " 'ManualSnapshotRetentionPeriod': -1,\n",
       " 'ClusterSecurityGroups': [],\n",
       " 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-019036ec555f0d73b',\n",
       "   'Status': 'active'}],\n",
       " 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "   'ParameterApplyStatus': 'in-sync'}],\n",
       " 'ClusterSubnetGroupName': 'default',\n",
       " 'VpcId': 'vpc-0af307b31fe59e41a',\n",
       " 'AvailabilityZone': 'us-east-2b',\n",
       " 'PreferredMaintenanceWindow': 'thu:06:00-thu:06:30',\n",
       " 'PendingModifiedValues': {},\n",
       " 'ClusterVersion': '1.0',\n",
       " 'AllowVersionUpgrade': True,\n",
       " 'NumberOfNodes': 1,\n",
       " 'PubliclyAccessible': True,\n",
       " 'Encrypted': False,\n",
       " 'ClusterPublicKey': 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDCC+3WpFirFD6jw1G0w5LMmQuyfAC+aQDmQMOA3/EeBHcuDfUtQl5eVQVNFQJQy45obZZYs20OZDprFgSfBZ8qCL2JMNLHzQYnvc5eN3kCMXrhKBA9D+/Uc20/X5U7fQoCKupELOhiVWIrM7M/iX3n5mhHRn0pAkcsb0+wzDngW5f1dZkFRAMX6cruMZTeBpG1HYLUi1qMbxYbHZdGvJggjuJs94gJnCDVtjH+RqCP7NTqw+lzOGImdyPf3kDkqLxduKx9jA+XEcKSZDhtd966oo2d+CBJyhf0y4tpoibUh1vWMsnnNtlUz/f9IYA35xsDbTE+5eMUVNOC7ZGGEd45 Amazon-Redshift\\n',\n",
       " 'ClusterNodes': [{'NodeRole': 'SHARED',\n",
       "   'PrivateIPAddress': '172.31.16.76',\n",
       "   'PublicIPAddress': '3.21.99.223'}],\n",
       " 'ClusterRevisionNumber': '71629',\n",
       " 'Tags': [{'Key': 'ENVIRONMENT', 'Value': 'DEV'}],\n",
       " 'EnhancedVpcRouting': False,\n",
       " 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::211125552279:role/redshift-tutorial-s3-access-role',\n",
       "   'ApplyStatus': 'in-sync'}],\n",
       " 'MaintenanceTrackName': 'current',\n",
       " 'DeferredMaintenanceWindows': [],\n",
       " 'NextMaintenanceWindowStartTime': datetime.datetime(2024, 8, 15, 6, 0, tzinfo=tzlocal()),\n",
       " 'AvailabilityZoneRelocationStatus': 'disabled',\n",
       " 'ClusterNamespaceArn': 'arn:aws:redshift:us-east-2:211125552279:namespace:f95f0fc3-d08c-42c8-a522-275e59cad67e',\n",
       " 'TotalStorageCapacityInMegaBytes': 400000,\n",
       " 'AquaConfiguration': {'AquaStatus': 'disabled',\n",
       "  'AquaConfigurationStatus': 'auto'},\n",
       " 'MultiAZ': 'Disabled'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshift_client.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterProps = get_redshift_props(redshift_client, DWH_CLUSTER_IDENTIFIER)\n",
    "if clusterProps:\n",
    "    prettyClusterProps = pretty_redshift_props(clusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>covid19-redshift-cluster-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>oseloka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>covid19-redshift-db-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'covid19-redshift-cluster-1.covkciolfldm.us-east-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-0af307b31fe59e41a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Parameter  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                                   value  \n",
       "0  covid19-redshift-cluster-1                                                                             \n",
       "1  dc2.large                                                                                              \n",
       "2  available                                                                                              \n",
       "3  oseloka                                                                                                \n",
       "4  covid19-redshift-db-1                                                                                  \n",
       "5  {'Address': 'covid19-redshift-cluster-1.covkciolfldm.us-east-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-0af307b31fe59e41a                                                                                  \n",
       "7  1                                                                                                      "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prettyClusterProps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = clusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = clusterProps['IamRoles'][0]['IamRoleArn']\n",
    "DB_NAME = clusterProps['DBName']\n",
    "DB_USER = clusterProps['MasterUsername']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-019036ec555f0d73b')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2_client.Vpc(id=clusterProps['VpcId'])\n",
    "    default_SG = list(vpc.security_groups.all())[0]\n",
    "    print(default_SG)\n",
    "\n",
    "    default_SG.authorize_ingress(\n",
    "        GroupName=default_SG.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DWH_ENDPOINT,\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DWH_DB_PASSWORD,\n",
    "        port=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur = conn.cursor()\n",
    "except Exception as e:\n",
    "    print(\"Error: Could not obtain database cursor\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(factCovid_sql)\n",
    "    cur.execute(dimHospital_sql)\n",
    "    cur.execute(dimDate_sql)\n",
    "    cur.execute(dimRegion_sql)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to copy 'factCovid' data from s3 into redshift\n",
      "The specified S3 prefix 'output//dimHospital.csv' does not exist\n",
      "DETAIL:  \n",
      "  -----------------------------------------------\n",
      "  error:  The specified S3 prefix 'output//dimHospital.csv' does not exist\n",
      "  code:      8001\n",
      "  context:   \n",
      "  query:     2822\n",
      "  location:  s3_utility.cpp:717\n",
      "  process:   padbmaster [pid=1073799386]\n",
      "  -----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cur.execute(\n",
    "    f\"\"\"\n",
    "    copy factCovid \n",
    "    from '{TARGET_OUTPUT_BUCKET}/dimHospital.csv'\n",
    "    credentials 'aws_iam_role={redshiftToS3_roleArn}'\n",
    "    delimiter ','\n",
    "    region '{TARGET_REGION}'\n",
    "    IGNOREHEADER 1\n",
    "    \"\"\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Unable to copy 'factCovid' data from s3 into redshift\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"select * from users;\")\n",
    "except Exception as e:\n",
    "    print(\"Unable to select from 'users' table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows =cur.fetchmany(10)\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn.close()\n",
    "except psycopg2.Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redshift_client.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
