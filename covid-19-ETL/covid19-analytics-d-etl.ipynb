{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import configparser\n",
    "from io import StringIO\n",
    "\n",
    "import boto3\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Join\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('covid19-analytics.config'))\n",
    "\n",
    "KEY = config.get('AWS', 'KEY')\n",
    "SECRET = config.get('AWS', 'SECRET')\n",
    "\n",
    "TARGET_OUTPUT_BUCKET=config.get('S3', 'TARGET_OUTPUT_BUCKET')\n",
    "TARGET_OUTPUT_S3 = config.get('S3', 'TARGET_OUTPUT_S3')\n",
    "TARGET_OUTPUT_DIR=config.get('S3', 'TARGET_OUTPUT_DIR')\n",
    "TARGET_REGION = config.get('S3', 'TARGET_REGION')\n",
    "TMP_DIR = config.get('FILE_PATHS', 'TMP_DIR')\n",
    "\n",
    "DWH_CLUSTER_TYPE = config.get('DWH', 'DWH_CLUSTER_TYPE')\n",
    "DWH_NUM_NODES = config.get('DWH', 'DWH_NUM_NODES')\n",
    "DWH_NODE_TYPE = config.get('DWH', 'DWH_NODE_TYPE')\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_DB = config.get('DWH', 'DWH_DB')\n",
    "DWH_DB_USER = config.get('DWH', 'DWH_DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('DWH', 'DWH_DB_PASSWORD')\n",
    "DWH_PORT = config.get('DWH', 'DWH_PORT')\n",
    "DWH_IAM_ROLE_NAME = config.get('DWH', 'DWH_IAM_ROLE_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_S3_CLIENT = boto3.client(\n",
    "    's3', \n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY, \n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "redshift_client = boto3.client(\n",
    "    'redshift',\n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "ec2_client = boto3.resource(\n",
    "    'ec2',\n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "iam_client = boto3.client(\n",
    "    'iam',\n",
    "    region_name=TARGET_REGION,\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma_jhu = pd.read_csv(f'{TMP_DIR}/enigma_jhu.csv')\n",
    "testing_data_states_daily = pd.read_csv(f'{TMP_DIR}/testing-datastates_daily.csv')\n",
    "\n",
    "factCovid_1 = enigma_jhu[['fips', 'province_state', 'country_region', 'confirmed', 'deaths', 'recovered', 'active' ]]\n",
    "factCovid_2 = testing_data_states_daily[['fips', 'date', 'positive', 'negative', 'hospitalizedcurrently', 'hospitalized', 'hospitalizeddischarged' ]]\n",
    "factCovid = pd.merge(factCovid_1, factCovid_2, on='fips', how='inner')\n",
    "print(len(factCovid))\n",
    "\n",
    "factCovid = factCovid.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimHospital = pd.read_csv(f'{TMP_DIR}/hospital-bedsjson.csv')\n",
    "dimHospital =  dimHospital[['fips', 'state_name', 'latitude', 'longtitude', 'hq_address', 'hospital_name', 'hospital_type', 'hq_city', 'hq_state']]\n",
    "dimHospital = dimHospital.rename(columns={'longtitude': 'longitude'})\n",
    "\n",
    "dimHospital = dimHospital.drop_duplicates()\n",
    "\n",
    "pd.to_numeric(dimHospital['latitude'], errors= 'coerce')\n",
    "pd.to_numeric(dimHospital['longitude'], errors= 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate = pd.read_csv(f'{TMP_DIR}/testing-datastates_daily.csv')\n",
    "dimDate = dimDate[['fips', 'date']]\n",
    "\n",
    "dimDate['date'] = pd.to_datetime(dimDate['date'], format='%Y%m%d')\n",
    "dimDate['year'] = dimDate['date'].dt.year\n",
    "dimDate['month'] = dimDate['date'].dt.month\n",
    "dimDate[\"day_of_week\"] = dimDate['date'].dt.dayofweek\n",
    "\n",
    "dimDate = dimDate.drop_duplicates()\n",
    "\n",
    "dimDate['fips'] = dimDate['fips'].astype(float)\n",
    "pd.to_datetime(dimDate['date'], errors= 'coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma_jhu = spark.read.csv(\n",
    "    f'{TMP_DIR}/enigma_jhu.csv', \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "ny_times_us_county = spark.read.csv(\n",
    "    f'{TMP_DIR}/us_county.csv', \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion_1 = enigma_jhu.select('fips', 'province_state', 'country_region', 'latitude', 'longitude')\n",
    "dimRegion_2 = ny_times_us_county.select('fips', 'county', 'state')\n",
    "\n",
    "dimRegion_1 = dimRegion_1.repartition(4, 'fips')\n",
    "dimRegion_2 = dimRegion_2.repartition(4, 'fips')\n",
    "dimRegion_2 = dimRegion_2.withColumnRenamed('fips', 'fips2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion = dimRegion_1.join(\n",
    "    dimRegion_2, \n",
    "    dimRegion_1[\"fips\"] == dimRegion_2[\"fips2\"], \n",
    "    \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion = dimRegion.drop('fips2')\n",
    "print(dimRegion.count())\n",
    "\n",
    "dimRegion = dimRegion.distinct()\n",
    "print(dimRegion.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion = dimRegion.toPandas()\n",
    "dimRegion['fips'] = dimRegion['fips'].astype(float)\n",
    "\n",
    "pd.to_numeric(dimRegion['latitude'], errors= 'coerce')\n",
    "pd.to_numeric(dimRegion['longitude'], errors= 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factCovid.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/factCovid.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimHospital.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimHospital.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimDate.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion.to_csv(csv_buffer)\n",
    "\n",
    "OUTPUT_S3_CLIENT.put_object(\n",
    "    Bucket=TARGET_OUTPUT_S3,\n",
    "    Key=f'{TARGET_OUTPUT_DIR}/dimRegion.csv',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    ContentType='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %rm -r -f {TMP_DIR}/* # Cleanup tmp directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct CREATE TABLE SQL dynamically from pandas dataframe\n",
    "factCovid_sql = f\"{pd.io.sql.get_schema(factCovid.reset_index(), 'factCovid')};\"\n",
    "staging_factCovid_sql =  f\"{pd.io.sql.get_schema(factCovid.reset_index(), 'staging_factCovid')};\"\n",
    "print(factCovid_sql)\n",
    "\n",
    "dimHospital_sql = f\"{pd.io.sql.get_schema(dimHospital.reset_index(), 'dimHospital')};\"\n",
    "staging_dimHospital_sql = f\"{pd.io.sql.get_schema(dimHospital.reset_index(), 'staging_dimHospital')};\"\n",
    "print(dimHospital_sql)\n",
    "\n",
    "dimDate_sql = f\"{pd.io.sql.get_schema(dimDate.reset_index(), 'dimDate')};\"\n",
    "staging_dimDate_sql = f\"{pd.io.sql.get_schema(dimDate.reset_index(), 'staging_dimDate')};\"\n",
    "print(dimDate_sql)\n",
    "\n",
    "dimRegion_sql = f\"{pd.io.sql.get_schema(dimRegion.reset_index(), 'dimRegion')};\"\n",
    "staging_dimRegion_sql = f\"{pd.io.sql.get_schema(dimRegion.reset_index(), 'staging_dimRegion')};\"\n",
    "print(dimRegion_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method implements retries while obtaining redshift properties in case creating cluster is yet complete\n",
    "def get_redshift_props(redshift_client, cluster_identifier):\n",
    "    retries = 30\n",
    "    retry_delay = 30 # Delay between retries in seconds\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            clusterProps = redshift_client.describe_clusters(ClusterIdentifier=cluster_identifier)['Clusters'][0]\n",
    "            if clusterProps['ClusterAvailabilityStatus'] == 'Available':\n",
    "                return clusterProps\n",
    "            elif clusterProps['ClusterAvailabilityStatus'] != 'Available':\n",
    "                if attempt < retries -1:\n",
    "                    print(f\"Cluster '{cluster_identifier}' not ready. Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "        except redshift_client.exceptions.ClusterNotFoundFault as e:\n",
    "            if attempt < retries -1:\n",
    "                print(f\"Cluster '{cluster_identifier}' not found. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay/3)\n",
    "            else:\n",
    "                raise e # Raise the last exception if the retries are exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_redshift_props(props):\n",
    "    pd.set_option('display.max.colwidth', 0)\n",
    "    keysToShow = ['ClusterIdentifier', 'ClusterStatus', 'NodeType', 'NumberOfNodes', 'DBName', 'MasterUsername', 'Endpoint', 'VpcId']\n",
    "    x = [(k, v) for k, v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=['Parameter', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterProps = get_redshift_props(redshift_client, DWH_CLUSTER_IDENTIFIER)\n",
    "if clusterProps:\n",
    "    prettyClusterProps = pretty_redshift_props(clusterProps)\n",
    "    DWH_ENDPOINT = clusterProps['Endpoint']['Address']\n",
    "    DWH_ROLE_ARN = clusterProps['IamRoles'][0]['IamRoleArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyClusterProps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2_client.Vpc(id=clusterProps['VpcId'])\n",
    "    default_SG = list(vpc.security_groups.all())[0]\n",
    "    print(default_SG)\n",
    "\n",
    "    default_SG.authorize_ingress(\n",
    "        GroupName=default_SG.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT),\n",
    "    )\n",
    "except ClientError as e:\n",
    "    # Check for duplicate rule errors\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == 'InvalidPermission.Duplicate':\n",
    "        print('Security group rule exists, no further actions required')\n",
    "    else:\n",
    "        raise e\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DWH_ENDPOINT,\n",
    "        dbname=DWH_DB,\n",
    "        user=DWH_DB_USER,\n",
    "        password=DWH_DB_PASSWORD,\n",
    "        port=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur = conn.cursor()\n",
    "except Exception as e:\n",
    "    print(\"Error: Could not obtain database cursor\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "try:\n",
    "    cur.execute(staging_factCovid_sql)\n",
    "    cur.execute(factCovid_sql)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    cur.execute(staging_dimHospital_sql)\n",
    "    cur.execute(dimHospital_sql)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    cur.execute(staging_dimDate_sql)\n",
    "    cur.execute(dimDate_sql)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    cur.execute(staging_dimRegion_sql)\n",
    "    cur.execute(dimRegion_sql)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factCovid.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimHospital.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\n",
    "    f\"\"\"\n",
    "    copy staging_dimhospital\n",
    "    from '{TARGET_OUTPUT_BUCKET}dimHospital.csv'\n",
    "    credentials 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "    delimiter ','\n",
    "    region '{TARGET_REGION}'\n",
    "    IGNOREHEADER 1\n",
    "    EMPTYASNULL\n",
    "    BLANKSASNULL\n",
    "    \"\"\"\n",
    "    )\n",
    "except ClientError as error:\n",
    "    print(error)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\n",
    "    f\"\"\"\n",
    "    copy staging_factCovid\n",
    "    from '{TARGET_OUTPUT_BUCKET}factCovid.csv'\n",
    "    credentials 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "    delimiter ','\n",
    "    region '{TARGET_REGION}'\n",
    "    IGNOREHEADER 1\n",
    "    EMPTYASNULL\n",
    "    BLANKSASNULL\n",
    "    \"\"\"\n",
    "    )\n",
    "except ClientError as error:\n",
    "    print(error)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\n",
    "    f\"\"\"\n",
    "    copy staging_dimdate\n",
    "    from '{TARGET_OUTPUT_BUCKET}dimDate.csv'\n",
    "    credentials 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "    delimiter ','\n",
    "    region '{TARGET_REGION}'\n",
    "    IGNOREHEADER 1\n",
    "    EMPTYASNULL\n",
    "    BLANKSASNULL\n",
    "    \"\"\"\n",
    "    )\n",
    "except ClientError as error:\n",
    "    print(error)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\n",
    "    f\"\"\"\n",
    "    copy staging_dimRegion\n",
    "    from '{TARGET_OUTPUT_BUCKET}dimRegion.csv'\n",
    "    credentials 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "    delimiter ','\n",
    "    region '{TARGET_REGION}'\n",
    "    IGNOREHEADER 1\n",
    "    EMPTYASNULL\n",
    "    BLANKSASNULL\n",
    "    \"\"\"\n",
    "    )\n",
    "except ClientError as error:\n",
    "    print(error)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in factCovid.columns if col != 'index']\n",
    "\n",
    "\n",
    "select_cols = ({','.join(columns)})\n",
    "select_sub = {','.join([f'sub.{col}' for col in columns])}\n",
    "\n",
    "print(select_cols)\n",
    "print(select_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting unique 'dimHospital' records using all columes exclusing 'index' to verify uniqueness\n",
    "columns = [col for col in dimHospital.columns if col != 'index']\n",
    "\n",
    "insert_dimHospital = f\"\"\"\n",
    "insert into dimHospital ({','.join(columns)})\n",
    "select {','.join([f'sub.{col}' for col in columns])}\n",
    "from (\n",
    "    select {','.join(columns)},\n",
    "        row_number() over (partition by {','.join(columns)} order by index) as row_num\n",
    "    from staging_dimHospital\n",
    ") sub\n",
    "where row_num = 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Inserting unique 'dimDate' records using all columes exclusing 'index' to verify uniqueness\n",
    "columns = [col for col in dimDate.columns if col != 'index']\n",
    "\n",
    "insert_dimDate = f\"\"\"\n",
    "insert into dimDate ({','.join(columns)})\n",
    "select {','.join([f'sub.{col}' for col in columns])}\n",
    "from (\n",
    "    select {','.join(columns)},\n",
    "        row_number() over (partition by {','.join(columns)} order by index) as row_num\n",
    "    from staging_dimDate\n",
    ") sub\n",
    "where row_num = 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Inserting unique 'dimRegion' records using all columes exclusing 'index' to verify uniqueness\n",
    "columns = [col for col in dimRegion.columns if col != 'index']\n",
    "\n",
    "insert_dimRegion = f\"\"\"\n",
    "insert into dimRegion ({','.join(columns)})\n",
    "select {','.join([f'sub.{col}' for col in columns])}\n",
    "from (\n",
    "    select {','.join(columns)},\n",
    "        row_number() over (partition by {','.join(columns)} order by index) as row_num\n",
    "    from staging_dimRegion\n",
    ") sub\n",
    "where row_num = 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Inserting unique 'factCovid' records using all columes exclusing 'index' to verify uniqueness\n",
    "columns = [col for col in factCovid.columns if col != 'index']\n",
    "\n",
    "insert_factCovid = f\"\"\"\n",
    "insert into factCovid ({','.join(columns)})\n",
    "select {','.join([f'sub.{col}' for col in columns])}\n",
    "from (\n",
    "    select {','.join(columns)},\n",
    "    from staging_factCovid\n",
    ") sub\n",
    "where row_num = 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(insert_factCovid)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [desc[0] for desc in cur.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows =cur.fetchmany(10)\n",
    "print(column_names)\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"select * from users;\")\n",
    "except Exception as e:\n",
    "    print(\"Unable to select from 'users' table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows =cur.fetchmany(10)\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "try:\n",
    "    conn.close()\n",
    "except psycopg2.Error as e:\n",
    "    print(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redshift_client.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
